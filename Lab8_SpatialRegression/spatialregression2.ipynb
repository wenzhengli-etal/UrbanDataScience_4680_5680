{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-gambling",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a7a244",
   "metadata": {},
   "source": [
    "# Learning goals\n",
    "After today's lesson you should be able to:\n",
    "- Understand the differences between SLX, spatial lag and spatial error models\n",
    "- Identify when to use different kinds of models\n",
    "- Implement all three types of models in pysal\n",
    "- Check our model outcomes \n",
    "\n",
    "This week's lesson is a simplied version of:  \n",
    "- The [Week 11 on Spatial Regression the Geographic Data Science book](https://geographicdata.science/book/notebooks/11_regression.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-introduction",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from pysal.lib import weights\n",
    "from pysal.explore import esda\n",
    "import numpy\n",
    "import pandas\n",
    "import geopandas\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-power",
   "metadata": {},
   "source": [
    "## Data: San Diego Airbnb\n",
    "\n",
    "To learn a little more about how regression works, we'll examine information about Airbnb properties in San Diego, CA. \n",
    "This dataset contains house intrinsic characteristics, both continuous (number of beds as in `beds`) and categorical (type of renting or, in Airbnb jargon, property group as in the series of `pg_X` binary variables), but also variables that explicitly refer to the location and spatial configuration of the dataset (e.g., distance to Balboa Park, `d2balboa` or neighborhood id, `neighborhood_cleansed`).\n",
    "\n",
    "Also, note that there is a great notebook in the Geographic Data Science book on [how this dataset was cleaned](https://geographicdata.science/book/data/airbnb/regression_cleaning.html). It has some example code for: \n",
    "- How to calculate the driving distance to a certain location (Balboa Park in this example) using a free API called Nomatim, since we only learned about the Google Maps API. \n",
    "- How to get the elevation of a location\n",
    "- How to categorize neighborhoods (here, by size and whether they are coastal).\n",
    "- Creating dummy variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = geopandas.read_file(\"https://www.dropbox.com/s/zkucu7jf1xug869/regression_db.geojson?dl=1\")\n",
    "# db['pool'] = db['pool'].astype(int)\n",
    "# db['coastal'] = db['coastal'].astype(int)\n",
    "# db['pg_Apartment'] = db['pg_Apartment'].astype(int)\n",
    "# db['pg_Condominium'] = db['pg_Condominium'].astype(int)\n",
    "# db['pg_House'] = db['pg_House'].astype(int)\n",
    "# db['pg_Other'] = db['pg_Other'].astype(int)\n",
    "# db['pg_Townhouse'] = db['pg_Townhouse'].astype(int)\n",
    "# db['rt_Entire_home/apt'] = db['rt_Entire_home/apt'].astype(int)\n",
    "# db['rt_Private_room'] = db['rt_Private_room'].astype(int)\n",
    "# db['rt_Shared_room'] = db['rt_Shared_room'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6174fca9",
   "metadata": {},
   "source": [
    "Notice here that we have: \n",
    "- **Discrete variables** (number of bedrooms, beds, baths)\n",
    "- **Dummy variables** (whether there is a pool, whether near the coast, room type)\n",
    "\n",
    "**Remember that for dummy variables we always run the regression leaving out one category as our baseline.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab78024",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9067357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-collins",
   "metadata": {},
   "source": [
    "These are the explanatory variables we will use throughout the chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_names = [\n",
    "    \"accommodates\",  # Number of people it accommodates\n",
    "    \"bathrooms\",  # Number of bathrooms\n",
    "    \"bedrooms\",  # Number of bedrooms\n",
    "    \"beds\",  # Number of beds\n",
    "    # Below are binary variables, 1 True, 0 False\n",
    "    \"rt_Private_room\",  # Room type: private room\n",
    "    \"rt_Shared_room\",  # Room type: shared room\n",
    "    \"pg_Condominium\",  # Property group: condo\n",
    "    \"pg_House\",  # Property group: house\n",
    "    \"pg_Other\",  # Property group: other\n",
    "    \"pg_Townhouse\",  # Property group: townhouse\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-block",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from pysal.model import spreg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-enlargement",
   "metadata": {},
   "source": [
    "### Spatial heterogeneity\n",
    "\n",
    "#### Spatial fixed effects\n",
    "Mathematically, we are now fitting the following equation:\n",
    "\n",
    "$$\n",
    "\\log{P_i} = \\alpha_r + \\sum_k \\mathbf{X}_{ik}\\beta_k  + \\epsilon_i\n",
    "$$\n",
    "\n",
    "where the main difference is that we are now allowing the constant term, $\\alpha$, to vary by neighborhood $r$, $\\alpha_r$.\n",
    "\n",
    "Programmatically, we will show two different ways we can estimate this: one,\n",
    "using `statsmodels`; and two, with `spreg`. First, we will use `statsmodels`, the econometrician's toolbox in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-wages",
   "metadata": {},
   "source": [
    "This package provides a formula-like API, which allows us to express the *equation* we wish to estimate directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = (\n",
    "    \"log_price ~ \"\n",
    "    + \" + \".join(variable_names)\n",
    "    + \" + neighborhood - 1\"\n",
    ")\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-limit",
   "metadata": {},
   "source": [
    "The *tilde* operator in this statement is usually read as \"log price is a function of ...\", to account for the fact that many different model specifications can be fit according to that functional relationship between `log_price` and our covariate list. **Critically, note that the trailing `-1` term means that we are fitting this model without an intercept term. This is necessary, since including an intercept term alongside unique means for every neighborhood would make the underlying system of equations underspecified.**\n",
    "\n",
    "Using this expression, we can estimate the unique effects of each neighborhood, fitting the model in `statsmodels` (note how the specification of the model, formula and data is separated from the fitting step): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-bahrain",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m3 = sm.ols(f, data=db).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-rental",
   "metadata": {},
   "source": [
    "We could rely on the `summary2()` method to print a similar summary report from the regression but, given it is a lengthy one in this case, we will illustrate how you can extract the spatial fixed effects into a table for display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-grant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store variable names for all the spatial fixed effects\n",
    "sfe_names = [i for i in m3.params.index if \"neighborhood[\" in i]\n",
    "# Create table\n",
    "pandas.DataFrame(\n",
    "    {\n",
    "        \"Coef.\": m3.params[sfe_names],\n",
    "        \"Std. Error\": m3.bse[sfe_names],\n",
    "        \"P-Value\": m3.pvalues[sfe_names],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-drove",
   "metadata": {},
   "source": [
    "The approach above shows how spatial FE are a particular case of a linear regression with a categorical  variable. Neighborhood membership is modeled using binary dummy variables. Thanks to the formula grammar used in `statsmodels`, we can express the model abstractly, and Python parses it, appropriately creating binary variables as required.\n",
    "\n",
    "The second approach leverages `spreg` Regimes functionality. We will see regimes below but, for now, think of them as a generalization of spatial fixed effects where not only $\\alpha$ can vary. This framework allows the user to specify which variables are to be estimated separately for each group. In this case, instead of describing the model in a formula, we need to pass each element of the model as separate arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spreg spatial fixed effect implementation\n",
    "## Note that we are using the new function OLS_Regimes\n",
    "\n",
    "m4 = spreg.OLS_Regimes(\n",
    "    # Dependent variable\n",
    "    db[[\"log_price\"]].values,\n",
    "    # Independent variables\n",
    "    db[variable_names].values,\n",
    "    # Variable specifying neighborhood membership\n",
    "    db[\"neighborhood\"].tolist(),\n",
    "    # Allow the constant term to vary by group/regime\n",
    "    constant_regi=\"many\",\n",
    "    # Variables to be allowed to vary (True) or kept\n",
    "    # constant (False). Here we set all to False\n",
    "    cols2regi=[False] * len(variable_names),\n",
    "    # If True, a separate regression is run for each regime.\n",
    "    regime_err_sep=False,\n",
    "    # Dependent variable name\n",
    "    name_y=\"log_price\",\n",
    "    # Independent variables names\n",
    "    name_x=variable_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3512c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m4.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-province",
   "metadata": {},
   "source": [
    "Similarly as above, we could rely on the `summary` attribute to print a report with all the results computed. For simplicity here, we will only confirm that, to the 12th decimal, the parameters estimated are indeed the same as those we get from `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-variable",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "numpy.round(m4.betas.flatten() - m3.params.values, decimals=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-creation",
   "metadata": {},
   "source": [
    "To make a map of neighborhood fixed effects, we need to process the results from our model slightly.\n",
    "\n",
    "First, we extract only the effects pertaining to the neighborhoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-evanescence",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_effects = m3.params.filter(like=\"neighborhood\")\n",
    "neighborhood_effects.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-desert",
   "metadata": {},
   "source": [
    "Then, we need to extract just the neighborhood name from the index of this Series. A simple way to do this is to strip all the characters that come before and after our neighborhood names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-madrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequence with the variable names without\n",
    "# `neighborhood[` and `]`\n",
    "stripped = neighborhood_effects.index.str.strip(\n",
    "    \"neighborhood[\"\n",
    ").str.strip(\"]\")\n",
    "# Reindex the neighborhood_effects Series on clean names\n",
    "neighborhood_effects.index = stripped\n",
    "# Convert Series to DataFrame\n",
    "neighborhood_effects = neighborhood_effects.to_frame(\"fixed_effect\")\n",
    "# Print top of table\n",
    "neighborhood_effects.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-sector",
   "metadata": {},
   "source": [
    "Good, we're back to our raw neighborhood names. These allow us to join it to an auxillary file with neighborhood boundaries that is indexed on the same names. Let's read the boundaries first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-appearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_path = 'http://data.insideairbnb.com/united-states/ca/san-diego/2023-12-04/visualisations/neighbourhoods.geojson'\n",
    "neighborhoods = geopandas.read_file(sd_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-reputation",
   "metadata": {},
   "source": [
    "And we can then merge the spatial FE and plot them on a map (Fig. XXX7XXX):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-lease",
   "metadata": {
    "caption": "Neighborhood effects on Airbnb nightly prices. Neighborhoods shown in grey are 'not statistically significant' in their effect on Airbnb prices.",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Merge SFE estimates (note not every polygon\n",
    "# receives an estimate since not every polygon\n",
    "# contains Airbnb properties)\n",
    "neighborhoods.merge(\n",
    "    neighborhood_effects,\n",
    "    how=\"left\",\n",
    "    left_on=\"neighbourhood\",\n",
    "    right_index=True\n",
    "    # Drop polygons without a SFE estimate\n",
    ").dropna(\n",
    "    subset=[\"fixed_effect\"]\n",
    "    # Plot quantile choropleth\n",
    ").explore(\n",
    "    \"fixed_effect\",  # Variable to display\n",
    "    scheme=\"quantiles\",  # Choropleth scheme\n",
    "    k=7,  # No. of classes in the choropleth\n",
    "    # linewidth=0.1,  # Polygon border width\n",
    "    cmap=\"viridis\",  # Color scheme\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-edmonton",
   "metadata": {},
   "source": [
    "We can see a clear spatial structure in the SFE estimates. The most expensive neighborhoods tend to be located near the coast, while the cheapest ones are more inland."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-generic",
   "metadata": {},
   "source": [
    "#### Spatial regimes\n",
    "The equation we will be estimating is:\n",
    "\n",
    "$$\n",
    "\\log{P_i} = \\alpha_r + \\sum_k \\mathbf{X}_{ki}\\beta_{k-r} + \\epsilon_i\n",
    "$$\n",
    "\n",
    "where we are not only allowing the constant term to vary by region ($\\alpha_r$), but also every other parameter ($\\beta_{k-r}$).\n",
    "\n",
    "To illustrate this approach, we will use the \"spatial differentiator\" of whether a house is in a coastal neighborhood or not (`coastal_neig`) to define the regimes. The rationale behind this choice is that renting a house close to the ocean might be a strong enough pull that people might be willing to pay at different *rates* for each of the house's characteristics.\n",
    "\n",
    "To implement this in Python, we use the `OLS_Regimes` class in `spreg`, which does most of the heavy lifting for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-ecology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pysal spatial regimes implementation\n",
    "m5 = spreg.OLS_Regimes(\n",
    "    # Dependent variable\n",
    "    db[[\"log_price\"]].values,\n",
    "    # Independent variables\n",
    "    db[variable_names].values,\n",
    "    # Variable specifying neighborhood membership\n",
    "    db[\"coastal\"].tolist(),\n",
    "    # Allow the constant term to vary by group/regime\n",
    "    constant_regi=\"many\",\n",
    "    # Allow separate sigma coefficients to be estimated\n",
    "    # by regime (False so a single sigma)\n",
    "    regime_err_sep=False,\n",
    "    # Dependent variable name\n",
    "    name_y=\"log_price\",\n",
    "    # Independent variables names\n",
    "    name_x=variable_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6f7af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m5.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-germany",
   "metadata": {},
   "source": [
    "The result can be explored and interpreted similarly to the previous ones. If you inspect the `summary` attribute, you will find the parameters for each variable mostly conform to what you would expect, across both regimes. To compare them, we can plot them side-by-side on a bespoke table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results table\n",
    "res = pandas.DataFrame(\n",
    "    {\n",
    "        # Pull out regression coefficients and\n",
    "        # flatten as they are returned as Nx1 array\n",
    "        \"Coeff.\": m5.betas.flatten(),\n",
    "        # Pull out and flatten standard errors\n",
    "        \"Std. Error\": m5.std_err.flatten(),\n",
    "        # Pull out P-values from t-stat object\n",
    "        \"P-Value\": [i[1] for i in m5.t_stat],\n",
    "    },\n",
    "    index=m5.name_x,\n",
    ")\n",
    "# Coastal regime\n",
    "## Extract variables for the coastal regime\n",
    "coastal = [i for i in res.index if \"1_\" in i]\n",
    "## Subset results to coastal and remove the 1_ underscore\n",
    "coastal = res.loc[coastal, :].rename(lambda i: i.replace(\"1_\", \"\"))\n",
    "## Build multi-index column names\n",
    "coastal.columns = pandas.MultiIndex.from_product(\n",
    "    [[\"Coastal\"], coastal.columns]\n",
    ")\n",
    "# Non-coastal model\n",
    "## Extract variables for the non-coastal regime\n",
    "ncoastal = [i for i in res.index if \"0_\" in i]\n",
    "## Subset results to non-coastal and remove the 0_ underscore\n",
    "ncoastal = res.loc[ncoastal, :].rename(lambda i: i.replace(\"0_\", \"\"))\n",
    "## Build multi-index column names\n",
    "ncoastal.columns = pandas.MultiIndex.from_product(\n",
    "    [[\"Non-coastal\"], ncoastal.columns]\n",
    ")\n",
    "# Concat both models\n",
    "pandas.concat([coastal, ncoastal], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-rubber",
   "metadata": {},
   "source": [
    "An interesting question arises around the relevance of the regimes. *Are estimates for each variable across regimes statistically different?* For this, the model object also calculates for us what is called a **Chow test. This is a statistic that tests the null hypothesis that estimates from different regimes are undistinguishable. If we reject the null, we have evidence suggesting the regimes actually make a difference.**\n",
    "\n",
    "Results from the Chow test are available on the `summary` attribute, or we can extract them directly from the model object, which we will do here. There are two types of Chow test. First is a global one that jointly tests for differences between the two regimes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "m5.chow.joint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-apparatus",
   "metadata": {},
   "source": [
    "The first value represents the statistic, while the second one captures the p-value. In this case, the two regimes are statistically different from each other. The next step then is to check whether each of the coefficients in our model differs across regimes. For this, we can pull them out into a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-rhythm",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.DataFrame(\n",
    "    # Chow results by variable\n",
    "    m5.chow.regi,\n",
    "    # Name of variables\n",
    "    index=m5.name_x_r,\n",
    "    # Column names\n",
    "    columns=[\"Statistic\", \"P-value\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-receipt",
   "metadata": {},
   "source": [
    "As we can see in the table, most variables do indeed differ across regimes, statistically speaking. This points to systematic differences in the data generating processes across spatial regimes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-latter",
   "metadata": {},
   "source": [
    "### Spatial dependence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-latino",
   "metadata": {},
   "source": [
    "#### Exogenous effects: The SLX model\n",
    "Now we estimate the following model:\n",
    "\n",
    "$$\n",
    "\\log(P_i) = \\alpha + \\sum^{p}_{k=1}X_{ij}\\beta_j + \\sum^{p}_{k=1}\\left(\\sum^{N}_{j=1}w_{ij}x_{jk}\\right)\\gamma_k + \\epsilon_i\n",
    "$$\n",
    "\n",
    "where $\\sum_{j=1}^N w_{ij}x_{jk}$ represents the spatial lag of the $k$ th explanatory variable.\n",
    "This can be stated in *matrix* form using the spatial weights matrix, $\\mathbf{W}$, as:\n",
    "\n",
    "$$\n",
    "\\log(P_i) = \\alpha + \\mathbf{X}\\beta + \\mathbf{WX}\\gamma + \\epsilon\n",
    "$$\n",
    "\n",
    "This splits the model to focus on two main effects: $\\beta$ and $\\gamma$. The\n",
    "$\\beta$ effect describes the change in $y_i$ when $X_{ik}$ changes by one. The subscript for site $i$ is important here: since we're dealing \n",
    "with a $\\mathbf{W}$ matrix, it's useful to be clear about where the change occurs. The $\\gamma$ effect represents the \n",
    "*indirect* association of a change in $X_i$ with the house price. \n",
    "\n",
    "This can be conceptualized in two ways. \n",
    "- First, one could think of $\\gamma$ as simply *the association between the price in a given house and a unit change in its average surroundings.*\n",
    "This is useful and simple. But this interpretation blurs *where* this change\n",
    "might occur. In truth, a change in a variable at site $i$ will result in a *spillover* to its surroundings:\n",
    "when $x_i$ changes, so too does the *spatial lag* of any site near $i$. \n",
    "The precise size of the change in the lag will depend on the structure of $\\mathbf{W}$, and it can be \n",
    "different for every site it is connected with. For example, think of a very highly connected \"focal\" site in a \n",
    "row-standardized weight matrix. This focal site will not be strongly affected \n",
    "if a neighbor changes by a single unit, since each site only contributes a \n",
    "small amount to the lag at the focal site. \n",
    "- Alternatively, consider a site with only \n",
    "one neighbor: its lag will change by *exactly* the amount its sole neighbor changes.\n",
    "Thus, to discover the exact indirect effect of a change $y$ caused by the change\n",
    "at a specific site $x_i$ you would need to compute the *change in the spatial lag*,\n",
    "and then use that as your *change* in $X$. We will discuss this in the following section. \n",
    "\n",
    "In Python, we can calculate the spatial lag of each variable whose name starts by `pg_`\n",
    "by first creating a list of all of those names, and then applying `pysal`'s\n",
    "`lag_spatial` to each of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-mediterranean",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = weights.KNN.from_dataframe(db, k=20)\n",
    "knn.transform = \"R\"\n",
    "\n",
    "# Select only columns in `db` containing the keyword `pg_`\n",
    "lag_variables_used = db.filter(\n",
    "        like=\"pg_\"\n",
    "        # Compute the spatial lag of each of those variables\n",
    "    )\n",
    "wx = lag_variables_used.apply(lambda y: weights.spatial_lag.lag_spatial(knn, y)\n",
    "        # Rename the spatial lag, adding w_ to the original name\n",
    "    )\n",
    "wx = wx.rename(columns=lambda c: \"w_\"+ c\n",
    "        # Remove the lag of the binary variable for apartments\n",
    "    )\n",
    "wx = wx.drop(\"w_pg_Apartment\", axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-irrigation",
   "metadata": {},
   "source": [
    "Once computed, we can run the model using OLS estimation because, in this\n",
    "context, the spatial  lags included do not violate any of the assumptions OLS\n",
    "relies on (they are essentially additional exogenous variables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge original variables with the spatial lags in `wx`\n",
    "slx_exog = db[variable_names].join(wx)\n",
    "# Fit linear model with `spreg`\n",
    "m6 = spreg.OLS(\n",
    "    # Dependent variable\n",
    "    db[[\"log_price\"]].values,\n",
    "    # Independent variables\n",
    "    slx_exog.values,\n",
    "    # Dependent variable name\n",
    "    name_y=\"l_price\",\n",
    "    # Independent variables names\n",
    "    name_x=slx_exog.columns.tolist(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f099a5-d48d-4869-bea0-01787d065e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m6.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-analyst",
   "metadata": {},
   "source": [
    "As in the previous cases, printing the `summary` attribute of the model object would show a full report table. The variables we included in the original regression\n",
    "display similar behavior, albeit with small changes in size, and can be\n",
    "interpreted also in a similar way. To focus on the aspects that differ from the previous models here, we will only pull out results for the variables for which we also included their spatial lags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect names of variables of interest\n",
    "vars_of_interest = (\n",
    "    db[variable_names].filter(like=\"pg_\").join(wx).columns\n",
    ")\n",
    "# Build full table of regression coefficients\n",
    "pandas.DataFrame(\n",
    "    {\n",
    "        # Pull out regression coefficients and\n",
    "        # flatten as they are returned as Nx1 array\n",
    "        \"Coeff.\": m6.betas.flatten(),\n",
    "        # Pull out and flatten standard errors\n",
    "        \"Std. Error\": m6.std_err.flatten(),\n",
    "        # Pull out P-values from t-stat object\n",
    "        \"P-Value\": [i[1] for i in m6.t_stat],\n",
    "    },\n",
    "    index=m6.name_x\n",
    "    # Subset for variables of interest only and round to\n",
    "    # four decimals\n",
    ").reindex(vars_of_interest).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-benjamin",
   "metadata": {},
   "source": [
    "The spatial lag of each type of property\n",
    "(`w_pg_XXX`) is the new addition. We observe that, except for the case\n",
    "of townhouses (same as with the binary variable, `pg_Townhouse`), they are all\n",
    "significant, suggesting our initial hypothesis on the role of the surrounding\n",
    "houses might indeed be at work here. \n",
    "\n",
    "As an illustration, let's look at some of the direct/indirect effects. \n",
    "The direct effect of the `pg_Condominium` variable means that condominiums are\n",
    "typically 11% more expensive ($\\beta_{pg\\_{Condominium}}=0.1063$) than the benchmark\n",
    "property type, apartments. More relevant to this section, any given house surrounded by \n",
    "condominiums *also* receives a price premium. But, since $pg_{Condominium}$ is a dummy variable,\n",
    "the spatial lag at site $i$ represents the *percentage* of properties near $i$ that are\n",
    "condominiums, which is between $0$ and $1$.\n",
    "So, a *unit* change in this variable means that you would increase the condominium \n",
    "percentage by 100%. Thus, a $1$ increase in `w_pg_Condominium` (a change of 100% percentage points)\n",
    "would result in a 59.2% increase in the property house price ($\\beta_{w_pg\\_Condominium} = 0.5928$). \n",
    "Similar interpretations can be derived for all other spatially lagged variables to derive the\n",
    "*indirect* effect of a change in the spatial lag. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-voluntary",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Spatial error\n",
    "\n",
    "The spatial error model includes a spatial lag in the *error* term of the equation:\n",
    "\n",
    "$$\n",
    "\\log{P_i} = \\alpha + \\sum_k \\beta_k X_{ki} + u_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "u_i = \\lambda u_{lag-i} + \\epsilon_i\n",
    "$$\n",
    "\n",
    "where $u_{lag-i} = \\sum_j w_{i,j} u_j$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit spatial error model with `spreg`\n",
    "# (GMM estimation allowing for heteroskedasticity)\n",
    "m7 = spreg.GM_Error_Het(\n",
    "    # Dependent variable\n",
    "    db[[\"log_price\"]].values,\n",
    "    # Independent variables\n",
    "    db[variable_names].values,\n",
    "    # Spatial weights matrix\n",
    "    w=knn,\n",
    "    # Dependent variable name\n",
    "    name_y=\"log_price\",\n",
    "    # Independent variables names\n",
    "    name_x=variable_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-andorra",
   "metadata": {},
   "source": [
    "Similarly as before, the `summary` attribute will return a full-featured table of results. For the most part, it may be interpreted in similar ways to those above. The main difference is that, in this case, we can also recover an estimate and inference for the $\\lambda$ parameter in the error term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-folks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build full table of regression coefficients\n",
    "pandas.DataFrame(\n",
    "    {\n",
    "        # Pull out regression coefficients and\n",
    "        # flatten as they are returned as Nx1 array\n",
    "        \"Coeff.\": m7.betas.flatten(),\n",
    "        # Pull out and flatten standard errors\n",
    "        \"Std. Error\": m7.std_err.flatten(),\n",
    "        # Pull out P-values from t-stat object\n",
    "        \"P-Value\": [i[1] for i in m7.z_stat],\n",
    "    },\n",
    "    index=m7.name_x\n",
    "    # Subset for lambda parameter and round to\n",
    "    # four decimals\n",
    ").reindex([\"lambda\"]).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-assist",
   "metadata": {},
   "source": [
    "#### Spatial lag\n",
    "\n",
    "The spatial lag model introduces a spatial lag of the *dependent* variable. In the example we have covered, this would translate into:\n",
    "\n",
    "$$\n",
    "\\log{P_i} = \\alpha + \\rho \\log{P_{lag-i}} + \\sum_k \\beta_k X_{ki} + \\epsilon_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit spatial lag model with `spreg`\n",
    "# (GMM estimation)\n",
    "m8 = spreg.GM_Lag(\n",
    "    # Dependent variable\n",
    "    db[[\"log_price\"]].values,\n",
    "    # Independent variables\n",
    "    db[variable_names].values,\n",
    "    # Spatial weights matrix\n",
    "    w=knn,\n",
    "    # Dependent variable name\n",
    "    name_y=\"log_price\",\n",
    "    # Independent variables names\n",
    "    name_x=variable_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1493650-0fe3-41c8-8e40-791fcbd8b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m8.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-alpha",
   "metadata": {},
   "source": [
    "And let's summarize the coefficients in a table as before (usual disclaimer about the `summary` object applies):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build full table of regression coefficients\n",
    "pandas.DataFrame(\n",
    "    {\n",
    "        # Pull out regression coefficients and\n",
    "        # flatten as they are returned as Nx1 array\n",
    "        \"Coeff.\": m8.betas.flatten(),\n",
    "        # Pull out and flatten standard errors\n",
    "        \"Std. Error\": m8.std_err.flatten(),\n",
    "        # Pull out P-values from t-stat object\n",
    "        \"P-Value\": [i[1] for i in m8.z_stat],\n",
    "    },\n",
    "    index=m8.name_z\n",
    "    # Round to four decimals\n",
    ").round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-broadway",
   "metadata": {},
   "source": [
    "## Q.1\n",
    "\n",
    "One common kind of spatial econometric model is the \"Spatial Durbin Model,\" which combines the SLX model with the spatial lag model. Alternatively, the \"Spatial Durbin Error Model\" combines the SLX model with the spatial error model. \n",
    "\n",
    "- Fit a Spatial Durbin variant of the spatial models we have fit in this chapter. \n",
    "- Do these variants improve the model fit? \n",
    "- What happens to the spatial autocorrelation parameters ($\\rho$, $\\lambda$) when the SLX term is added? Why might this occur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26190d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
