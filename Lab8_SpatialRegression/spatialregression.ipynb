{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-gambling",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a7a244",
   "metadata": {},
   "source": [
    "# Learning goals\n",
    "After today's lesson you should be able to:\n",
    "- Understand the differences between SLX, spatial lag and spatial error models\n",
    "- Identify when to use different kinds of models\n",
    "- Implement all three types of models in pysal\n",
    "- Check our model outcomes \n",
    "\n",
    "This week's lesson is a simplied version of:  \n",
    "- The [Week 11 on Spatial Regression the Geographic Data Science book](https://geographicdata.science/book/notebooks/11_regression.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-introduction",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from pysal.lib import weights\n",
    "from pysal.explore import esda\n",
    "import numpy\n",
    "import pandas\n",
    "import geopandas\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-power",
   "metadata": {},
   "source": [
    "## Data: San Diego Airbnb\n",
    "\n",
    "To learn a little more about how regression works, we'll examine information about Airbnb properties in San Diego, CA. \n",
    "This dataset contains house intrinsic characteristics, both continuous (number of beds as in `beds`) and categorical (type of renting or, in Airbnb jargon, property group as in the series of `pg_X` binary variables), but also variables that explicitly refer to the location and spatial configuration of the dataset (e.g., distance to Balboa Park, `d2balboa` or neighborhood id, `neighborhood_cleansed`).\n",
    "\n",
    "Also, note that there is a great notebook in the Geographic Data Science book on [how this dataset was cleaned](https://geographicdata.science/book/data/airbnb/regression_cleaning.html). It has some example code for: \n",
    "- How to calculate the driving distance to a certain location (Balboa Park in this example) using a free API called Nomatim, since we only learned about the Google Maps API. \n",
    "- How to get the elevation of a location\n",
    "- How to categorize neighborhoods (here, by size and whether they are coastal).\n",
    "- Creating dummy variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = geopandas.read_file(\"https://www.dropbox.com/s/zkucu7jf1xug869/regression_db.geojson?dl=1\")\n",
    "# db['pool'] = db['pool'].astype(int)\n",
    "# db['coastal'] = db['coastal'].astype(int)\n",
    "# db['pg_Apartment'] = db['pg_Apartment'].astype(int)\n",
    "# db['pg_Condominium'] = db['pg_Condominium'].astype(int)\n",
    "# db['pg_House'] = db['pg_House'].astype(int)\n",
    "# db['pg_Other'] = db['pg_Other'].astype(int)\n",
    "# db['pg_Townhouse'] = db['pg_Townhouse'].astype(int)\n",
    "# db['rt_Entire_home/apt'] = db['rt_Entire_home/apt'].astype(int)\n",
    "# db['rt_Private_room'] = db['rt_Private_room'].astype(int)\n",
    "# db['rt_Shared_room'] = db['rt_Shared_room'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6174fca9",
   "metadata": {},
   "source": [
    "Notice here that we have: \n",
    "- **Discrete variables** (number of bedrooms, beds, baths)\n",
    "- **Dummy variables** (whether there is a pool, whether near the coast, room type)\n",
    "\n",
    "**Remember that for dummy variables we always run the regression leaving out one category as our baseline.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab78024",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9067357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-collins",
   "metadata": {},
   "source": [
    "These are the explanatory variables we will use throughout the chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_names = [\n",
    "    \"accommodates\",  # Number of people it accommodates\n",
    "    \"bathrooms\",  # Number of bathrooms\n",
    "    \"bedrooms\",  # Number of bedrooms\n",
    "    \"beds\",  # Number of beds\n",
    "    # Below are binary variables, 1 True, 0 False\n",
    "    \"rt_Private_room\",  # Room type: private room\n",
    "    \"rt_Shared_room\",  # Room type: shared room\n",
    "    \"pg_Condominium\",  # Property group: condo\n",
    "    \"pg_House\",  # Property group: house\n",
    "    \"pg_Other\",  # Property group: other\n",
    "    \"pg_Townhouse\",  # Property group: townhouse\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-telling",
   "metadata": {},
   "source": [
    "## Non-spatial regression, a (very) quick refresh\n",
    "For example, in our case, we may want to express the price of a house as a function of the number of bedrooms it has and whether it is a condominium or not. At the individual level, we can express this as:\n",
    "\n",
    "$$\n",
    "log(P_i) = \\alpha + \\sum_k \\mathbf{X}_{ik}\\beta_k  + \\epsilon_i\n",
    "$$\n",
    "\n",
    "where $P_i$ is the Airbnb price of house $i$, and $X$ is a set of covariates that we use to explain such price (e.g., No. of bedrooms and condominium binary variable). $\\beta$ is a vector of parameters that give us information about in which way and to what extent each variable is related to the price, and $\\alpha$, the constant term, is the average house price when all the other variables are zero. The term $\\epsilon_i$ is usually referred to as \"error\" and captures elements that influence the price of a house but are not included in $X$.\n",
    "\n",
    "We also take the log of prices often since a log scale allows us a better understanding of the percentage change instead of the absolute dollar change.  \n",
    "\n",
    "We can also express this relation in matrix form, excluding sub-indices for $i$, which yields:\n",
    "\n",
    "$$\n",
    "log(P) = \\alpha + \\mathbf{X}\\beta + \\epsilon\n",
    "$$\n",
    "\n",
    "Practically speaking, linear regressions in Python are rather streamlined and easy to work with. There are also several packages which will run them (e.g., `statsmodels`, `scikit-learn`, `pysal`). We will import the `spreg` module in Pysal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-block",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from pysal.model import spreg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-porter",
   "metadata": {},
   "source": [
    "In the context of this chapter, it makes sense to start with `spreg`, as that is the only library that will allow us to move into explicitly spatial econometric models. To fit the model specified in the equation above with $X$ as the list defined, using ordinary least squares (OLS), we only need the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit OLS model\n",
    "m1 = spreg.OLS(\n",
    "    # Dependent variable\n",
    "    db[[\"log_price\"]].values,\n",
    "    # Independent variables\n",
    "    db[variable_names].values,\n",
    "    # Dependent variable name\n",
    "    name_y=\"log_price\",\n",
    "    # Independent variable name\n",
    "    name_x=variable_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-procedure",
   "metadata": {},
   "source": [
    "We use the command `OLS`, part of the `spreg` sub-package, and specify the dependent variable (the log of the price, so we can interpret results in terms of percentage change) and the explanatory ones. Note that both objects need to be arrays, so we extract them from the `pandas.DataFrame` object using `.values`.\n",
    "\n",
    "In order to inspect the results of the model, we can print the `summary` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-research",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m1.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-scratch",
   "metadata": {},
   "source": [
    "Results are largely as expected: houses tend to be significantly more expensive if they accommodate more people (`accommodates`), if they have more bathrooms and bedrooms, and if they are a condominium or part of the \"other\" category of house type. Conversely, given a number of rooms, houses with more beds (i.e., listings that are more \"crowded\") tend to go for cheaper, as it is the case for properties where one does not rent the entire house but only a room (`rt_Private_room`) or even shares it (`rt_Shared_room`). Of course, you might conceptually doubt the assumption that it is possible to *arbitrarily* change the number of beds within an Airbnb without eventually changing the number of people it accommodates, but methods to address these concerns using *interaction effects* won't be discussed here. \n",
    "\n",
    "### Hidden structures\n",
    "\n",
    "In general, our model performs well, being able to predict slightly about two-thirds ($R^2=0.67$) of the variation in the mean nightly price using the covariates we've discussed above.\n",
    "But, our model might display some clustering in the errors, which may be a problem as that violates the i.i.d. assumption linear models usually come built-in with. \n",
    "To interrogate this, we can do a few things. \n",
    "One simple concept might be to look at the correlation between the error in predicting an Airbnb and the error in predicting its nearest neighbor. \n",
    "To examine this, we first might want to split our data up by regions and see if we've got some spatial structure in our residuals. \n",
    "One reasonable theory might be that our model does not include any information about *beaches*, a critical aspect of why people live and vacation in San Diego. \n",
    "Therefore, we might want to see whether or not our errors are higher or lower depending on whether or not an Airbnb is in a \"beach\" neighborhood, a neighborhood near the ocean. We use the code below to generate Figure XXX1XXX, which looks at prices between the two groups of houses, \"beach\" and \"no beach\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb874ea9",
   "metadata": {},
   "source": [
    "Note here that `m1.u` are the residuals from the model `m1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-above",
   "metadata": {
    "caption": "Distributions of prediction errors (residuals) for the basic linear model. Residuals for coastal Airbnbs are generally positive, meaning that the model under-predicts their prices.",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Boolean (True/False) with whether a\n",
    "# property is coastal or not\n",
    "is_coastal = db.coastal.astype(bool)\n",
    "# Split residuals (m1.u) between coastal and not\n",
    "coastal = m1.u[is_coastal]\n",
    "not_coastal = m1.u[~is_coastal]\n",
    "# Create histogram of the distribution of coastal residuals\n",
    "plt.hist(coastal, density=True, label=\"Coastal\")\n",
    "# Create histogram of the distribution of non-coastal residuals\n",
    "plt.hist(\n",
    "    not_coastal,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    linewidth=4,\n",
    "    label=\"Not Coastal\",\n",
    ")\n",
    "# Add Line on 0\n",
    "plt.vlines(0, 0, 1, linestyle=\":\", color=\"k\", linewidth=4)\n",
    "# Add legend\n",
    "plt.legend()\n",
    "# Display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-romantic",
   "metadata": {},
   "source": [
    "While it appears that the neighborhoods on the coast have only slightly higher average errors (and have lower variance in their prediction errors), the two distributions are significantly distinct from one another when compared using a classic $t$-test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-cinema",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "ttest_ind(coastal, not_coastal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201ea4ec",
   "metadata": {},
   "source": [
    "## Q1 \n",
    "What does the pvalue above tell us about how the errors in our data on the coast compared to neighborhoods not on the coast? Why are errors on the coast higher?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da26d451",
   "metadata": {},
   "source": [
    "INSERT YOUR TEXT HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-ranking",
   "metadata": {},
   "source": [
    "Additionally, it might be the case that some neighborhoods are more desirable than other neighborhoods due to unmodeled latent preferences or marketing. \n",
    "For instance, despite its presence close to the sea, living near Camp Pendleton -a Marine base in the North of the city- may incur some significant penalties on area desirability due to noise and pollution. These are questions that domain knowledge provides and data analysis can help us answer.\n",
    "For us to determine whether this is the case, we might be interested in the full distribution of model residuals within each neighborhood. \n",
    "\n",
    "To make this more clear, we'll first sort the data by the median residual in that neighborhood, and then make a boxplot (Fig. XXX2XXX), which shows the distribution of residuals in each neighborhood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-symposium",
   "metadata": {
    "caption": "Boxplot of prediction errors by neighborhood in San Diego, showing that the basic model systematically over- (or under-) predicts the nightly price of some neighborhoods' Airbnbs.",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create column with residual values from m1\n",
    "db[\"residual\"] = m1.u\n",
    "# Obtain the median value of residuals in each neighborhood\n",
    "medians = (\n",
    "    db.groupby(\"neighborhood\")\n",
    "    .residual.median()\n",
    "    .to_frame(\"hood_residual\")\n",
    ")\n",
    "\n",
    "# Increase fontsize\n",
    "seaborn.set(font_scale=1.25)\n",
    "# Set up figure\n",
    "f = plt.figure(figsize=(15, 3))\n",
    "# Grab figure's axis\n",
    "ax = plt.gca()\n",
    "# Generate bloxplot of values by neighborhood\n",
    "# Note the data includes the median values merged on-the-fly\n",
    "seaborn.boxplot(\n",
    "    data=db.merge(\n",
    "        medians, how=\"left\", left_on=\"neighborhood\", right_index=True\n",
    "    ).sort_values(\"hood_residual\"),\n",
    "    x=\"neighborhood\",\n",
    "    y=\"residual\",\n",
    "    ax=ax,\n",
    "    palette=\"bwr\",\n",
    ")\n",
    "# Auto-format of the X labels\n",
    "f.autofmt_xdate()\n",
    "# Display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-shock",
   "metadata": {},
   "source": [
    "No neighborhood is disjoint from one another, but some do appear to be higher than others, such as the well-known downtown tourist neighborhoods areas of the Gaslamp Quarter, Little Italy, or The Core. \n",
    "Thus, there may be a distinctive effect of intangible neighborhood fashionableness that matters in this model. \n",
    "\n",
    "Noting that many of the most over- and under-predicted neighborhoods are near one another in the city, it may also be the case that there is some sort of *contagion* or spatial spillovers in the nightly rent price. \n",
    "This often is apparent when individuals seek to price their Airbnb listings to compete with similar nearby listings. \n",
    "Since our model is not aware of this behavior, its errors may tend to cluster. \n",
    "**One exceptionally simple way we can look into this structure is by examining the relationship between an observation's residuals and its surrounding residuals.**\n",
    "\n",
    "To do this, we will use *spatial weights* to represent the geographic relationships between observations. \n",
    "We cover spatial weights in detail in [Chapter 4](04_spatial_weights), so we will not repeat ourselves here.\n",
    "For this example, we'll start off with a $KNN$ matrix where $k=1$, meaning we're focusing only on the linkages of each Airbnb to their closest other listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-maldives",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = weights.KNN.from_dataframe(db, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-billion",
   "metadata": {},
   "source": [
    "This means that, when we compute the *spatial lag* of that $KNN$ weight and the residual, we get the residual of the Airbnb listing closest to each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-termination",
   "metadata": {
    "caption": "The relationship between prediction error for an Airbnb and the nearest Airbnb's prediction error. This suggests that if an Airbnb's nightly price is over-predicted, its nearby Airbnbs will also be over-predicted.",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Here lag_spatial is a function that returns the spatial lag of a variable\n",
    "## given a certain set of weights\n",
    "\n",
    "lag_residual = weights.spatial_lag.lag_spatial(knn, m1.u)\n",
    "ax = seaborn.regplot(\n",
    "    x=m1.u.flatten(),\n",
    "    y=lag_residual.flatten(),\n",
    "    line_kws=dict(color=\"orangered\"),\n",
    "    scatter_kws={'alpha': 0.3,'s':10},\n",
    "    ci=None,\n",
    ")\n",
    "ax.set_xlabel(\"Model Residuals - $u$\")\n",
    "ax.set_ylabel(\"Spatial Lag of Model Residuals - $W u$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-salad",
   "metadata": {},
   "source": [
    "In Figure XXX3XXX, we see that our prediction errors tend to cluster!\n",
    "Above, we show the relationship between our prediction error at each site and the prediction error at the site nearest to it. \n",
    "Here, we're using this nearest site to stand in for the *surroundings* of that Airbnb. \n",
    "This means that, when the model tends to over-predict a given Airbnb's nightly log price, sites around that Airbnb are more likely to *also be over-predicted*. \n",
    "\n",
    "Let's look at the stable $k=20$ number of neighbors.\n",
    "Examining the relationship between this stable *surrounding* average and the focal Airbnb, we can even find clusters in our model error. \n",
    "Recalling the *local Moran* statistics in [Chapter 7](07_local_autocorrelation), Figure XXX4XXX is generated from the code below to identify certain areas where our predictions of the nightly (log) Airbnb price tend to be significantly off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-annotation",
   "metadata": {
    "caption": "Map of clusters in regression errors, according to the Local Moran's $I_i$.",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Re-weight W to 20 nearest neighbors\n",
    "knn20 = weights.KNN.from_dataframe(db, k=20)# Row standardize weights\n",
    "\n",
    "knn20.transform = \"R\"\n",
    "# Run LISA on residuals\n",
    "outliers = esda.moran.Moran_Local(m1.u, knn20, permutations=9999)\n",
    "# Select only LISA cluster cores\n",
    "error_clusters = outliers.q % 2 == 1\n",
    "# Filter out non-significant clusters\n",
    "error_clusters &= outliers.p_sim <= 0.001\n",
    "# Add `error_clusters` and `local_I` columns\n",
    "\n",
    "db = db.assign(\n",
    "        error_clusters=error_clusters,\n",
    "        local_I=outliers.Is\n",
    "        # Retain error clusters only\n",
    "    ).query(\n",
    "        \"error_clusters\"\n",
    "        # Sort by I value to largest plot on top\n",
    "    ).sort_values(\n",
    "        \"local_I\"\n",
    "        # Plot I values\n",
    "    )\n",
    " \n",
    "db.explore('local_I')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-examination",
   "metadata": {},
   "source": [
    "Thus, these areas tend to be locations where our model significantly under-predicts the nightly Airbnb price both for that specific observation and observations in its immediate surroundings. \n",
    "This is critical since, if we can identify how these areas are structured &mdash; if they have a *consistent geography* that we can model &mdash; then we might make our predictions even better, or at least not systematically mis-predict prices in some areas while correctly predicting prices in other areas. Since significant under- and over-predictions do appear to cluster in a highly structured way, we might be able to use a better model to fix the geography of our model errors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-understanding",
   "metadata": {},
   "source": [
    "### Spatial feature engineering: proximity variables\n",
    "One relevant proximity-driven variable that could influence our San Diego model is based on the listings proximity to Balboa Park. A common tourist destination, Balboa Park is a central recreation hub for the city of San Diego, containing many museums and the San Diego Zoo. Thus, it could be the case that people searching for Airbnbs in San Diego are willing to pay a premium to live closer to the park. If this were true *and* we omitted this from our model, we may indeed see a significant spatial pattern caused by this distance decay effect. \n",
    "\n",
    "Therefore, this is sometimes called a *spatially patterned omitted covariate*: geographic information our model needs to make good predictions which we have left out of our model. Therefore, let's build a new model containing this distance to Balboa Park covariate. First, though, it helps to visualize (Fig. XXX5XXX) the structure of this distance covariate itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d3ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.explore('d2balboa')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-insulin",
   "metadata": {},
   "source": [
    "To run a linear model that includes the additional variable of distance to the park, we add the name to the list of variables we included originally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "balboa_names = variable_names + [\"d2balboa\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-practitioner",
   "metadata": {},
   "source": [
    "And then fit the model using the OLS class in Pysal's `spreg`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = spreg.OLS(\n",
    "    db[[\"log_price\"]].values,\n",
    "    db[balboa_names].values,\n",
    "    name_y=\"log_price\",\n",
    "    name_x=balboa_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b323124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m2.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-bahrain",
   "metadata": {},
   "source": [
    "When you inspect the regression diagnostics and output, you see that this covariate is not quite as helpful as we might anticipate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.DataFrame(\n",
    "    [[m1.r2, m1.ar2], [m2.r2, m2.ar2]],\n",
    "    index=[\"M1\", \"M2\"],\n",
    "    columns=[\"R2\", \"Adj. R2\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-advertising",
   "metadata": {},
   "source": [
    "And, there still appears to be spatial structure in our model's errors, as we can see in Figure XXX6XXX, generated by the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-treat",
   "metadata": {
    "caption": "The relationship between prediction error and the nearest Airbnb's prediction error for the model including the 'Distance to Balboa Park' variable. Note the much stronger relationship here than before.",
    "tags": []
   },
   "outputs": [],
   "source": [
    "knn20 = weights.KNN.from_dataframe(db, k=20)# Row standardize weights\n",
    "knn20.transform = \"R\"\n",
    "\n",
    "lag_residual = weights.spatial_lag.lag_spatial(knn20, m2.u)\n",
    "ax = seaborn.regplot(\n",
    "    x=m2.u.flatten(),\n",
    "    y=lag_residual.flatten(),\n",
    "    line_kws=dict(color=\"orangered\"),\n",
    "    ci=None,\n",
    ")\n",
    "ax.set_xlabel(\"Residuals ($u$)\")\n",
    "ax.set_ylabel(\"Spatial lag of residuals ($Wu$)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-tuition",
   "metadata": {},
   "source": [
    "Finally, the distance to Balboa Park variable does not fit our theory about how distance to amenity should affect the price of an Airbnb; the coefficient estimate is *positive*, meaning that people are paying a premium to be *further* from Balboa Park. We will revisit this result later on, when we consider spatial heterogeneity and will be able to shed some light on this. Further, the next chapter is an extensive treatment of spatial fixed effects, presenting many more spatial feature engineering methods. Here, we have only showed how to include these engineered features in a standard linear modeling framework. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-broadway",
   "metadata": {},
   "source": [
    "## Q.2\n",
    "Run another model (m3), where we think distance to the airport might be an important factor in determining prices. What might be the relationship between distance to airport and price? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe2569d",
   "metadata": {},
   "source": [
    "INSERT YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd4f188",
   "metadata": {},
   "source": [
    "Now add a new column `d2airport` that has the distance between each Airbnb location and the airport 32.732346 (lattitude) -117.196053(longitude). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2b87d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75df3754",
   "metadata": {},
   "source": [
    "Now run and print the summary results of a model that includes the `variable_names` list, `d2balboa`, and `d2airport`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b137db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f779c7fc",
   "metadata": {},
   "source": [
    "Were you able to confirm your hypothesis? Why or why not? What might be the issue with including both `d2balboa`, and `d2airport` in your model? (Hint: look at the two locations on a map.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db574b73",
   "metadata": {},
   "source": [
    "INSERT YOUR TEXT HERE"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
